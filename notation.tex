\textbf{Notations and assumptions:} $\mathcal{A}$ denotes the set of arms, and $|\mathcal{A}|=K$ is the number of arms in $\mathcal{A}$. 
a generic arm is indexed by $i,j\in\mathcal{A}$. For arm $i\in\mathcal{A}$, we use $r_{i}$ to denote the true mean of the distribution from which the rewards are sampled, while $\hat{r}_{i}(t)$ denotes the estimated mean at time $t$. Formally, using $n_i(t)$ to denote the number of times arm $i$ has been pulled until time $t$, we have $\hat{r}_{i}(t)=\frac{1}{n_{i}(t)}\sum_{z=1}^{n_i(t)} X_{i,z}$, where $X_{i,z}$ is the reward sample received when arm $i$ is pulled for the $z$-th time. For simplicity, whenever there is no confusion about the time index $t$, we simply neglect the  denote $\hat{r}_i(t)$ and $\hat{r}_i$ and $n_i(t)=n_i$ whenever there is no confusion about the time index $t$ (this nomenclature also holds for the following notation). 
%
Similarly, for arm $i$ we use $\sigma_{i}^{2}$ to denote the true variance of the corresponding reward distribution, while $\hat{v}_{i}(t)$ is the estimated variance, i.e., $\hat{v}_{i}(t)=\frac{1}{n_i(t)}\sum_{z=1}^{n_{i}(t)}(X_{i,z}-\hat{r}_{i})^{2}$. Let  $\Delta_{i}=|\tau-r_{i}|$ and $\hat{\Delta}_{i}=|\tau-\hat{r}_{i}|$.


%The average estimated payoff for any arm is denoted by  whereas the true mean of the distribution  is denoted by . The optimal arm is denoted by $*$. The '*' superscript is used to denote anything related to optimal arm. 

 % $n_{i}$ denotes the number of times the arm $i$ has been pulled. $\psi $ denotes the exploration regulatory factor and $\rho_\mu ,\rho_v$ as arm elimination parameters. $\hat{V}_{i}=\frac{1}{n_i}\sum_{t=1}^{n_{i}}(x_{i,t}-r_{i})^{2}$ denotes the empirical variance and $x_{i,t}$ is the reward obtained at timestep t for arm $i$. Also   denotes the true variance of the arm $i$. 
 
 We assume that the distribution from which rewards are sampled are identical and independent 1-sub-Gaussian distributions which includes Gaussian distributions with variance less than 1 distributions supported on an interval of length less than 2. We will also assume that all rewards are bounded in $[0,1]$.

%Also we define $\Delta_{i}=r^{*} - r_{i}$ and $\hat{\Delta}_{i}=\hat{r}^{*} - \hat{r}_{i}$. In all cases $\min_{i\in A}{\Delta_{i}}$ is denoted by $\Delta$.
%and the optimal arm is denoted by $*$. The '*' superscript is used to denote anything related to optimal arm
%\paragraph*{}It is assumed that the distribution from which rewards are sampled are identical and independent sub-Gaussian distributions. Throughout the paper, we assume that the distributions $v_{i}$ are sub-Gaussian that is $\int e^{\lambda(x - r)} v_{i} (dx) â‰¤ e^{\lambda /2}, \forall \lambda \in \mathbb{R}$. Note that these include Gaussian distributions with variance less than 1 and distributions supported on an interval of length less than 2. All the experiments are also conducted with sub-Gaussians having variance as 1. Together with a Chernoff-Hoeffding bound, the sub-Gaussian assumption implies the following concentration inequality, valid for any
%$u > 0$,
%\newline
%\hspace*{8em}$\mathbb{P}\lbrace \hat{r}_{i} - r^{*} > u\rbrace \leq exp(-\dfrac{su^{2}}{2}) $
%\newline
%where s is the number of pulls of $a_{i}$. T is the horizon over which this entire algorithm runs.  $A^{'}$ at any round $m$ denotes the arms still not eliminated.
%\paragraph*{}The paper is organized as follows. We first present the algorithm in section 6. We then provide the proofs of Phase1 which includes regret-bound calculation and arm deletion conditions in section 7. In section 8, we provide proofs for Phase2 along with early stopping conditions. Section 9 deals with regret bound and then we provide error probability and error bounds in section 10. Experimental results are provided in section 11, and we conclude in section 12.