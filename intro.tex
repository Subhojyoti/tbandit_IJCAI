In this paper, we study a specific combinatorial pure exploration problem called thresholding bandit problem (TBP) in the stochastic multi-armed bandit (MAB) setting. MAB problems are classic sequential decision making problems where the learner is provided with a set of actions (or arms) whose rewards are i.i.d samples from the distribution specific to the arm $i\in A$ and whose expected mean is denoted by $r_{i},\forall i\in A$. The learner's job is to identify the best arm whose expected mean is denoted by $r^{*}$. So, at very timestep the learner selects an arm $i$ and hence faces the \emph{exploration-exploitation dilemma} whereby it could pull the arm which has the highest observed mean reward (or $\hat{r}_{i}$) till now (exploitation) or to explore other arms, with the prospect of finding superior performance which was previously unobserved (exploration).

%In the stochastic multi-armed bandit setting a learning agent is required to choose from a set of decisions or arms at every round. The agent is then presented with a reward for that round, which is an independent draw from a stationary distribution specific to the arm selected. The agent, however, does not know the mean of the distributions associated with each arm, denoted by $r_{i}$, including the optimal arm which will give it the best reward, denoted by $r^{*}$. The agent attempts to make arm choices that will maximize some performance measure by keeping track of the reward that has been gathered from previous selections of the arm, for each arm. This is called the estimated mean reward of an arm denoted by $\hat{r}_{i}$. The bandit problem can be conceptualized as a sequential decision making process where the agent is at each round presented with an \emph{exploration-exploitation dilemma}. The agent could pull the arm which has the highest observed mean reward till now (exploitation) or to explore other arms, with the prospect of finding superior performance which was previously unobserved (exploration). 

%
%	Formally, let $r_i$, $i=1,\ldots,K$ denote the mean rewards of the $K$ arms and $r^* = \max_i r_i$ the optimal mean reward. The objective in some of the stochastic bandit problem is to minimize the cumulative regret, which is defined as follows:
%\begin{align*}
%R_{T}=r^{*}T - \sum_{i\in A} r_{i}N_{i}(T),
%\end{align*}
%where $T$ is the number of rounds, $N_{i}(T)=\sum_{m=1}^T I(I_m=i)$ is the number of times the algorithm chose arm $i$ up to round $T$.
%The expected regret of an algorithm after $T$ rounds can be written as,
%
%\begin{align*}
%\E[R_{T}]= \sum_{i=1}^K \E[N_i(T)] \Delta_i,
%\end{align*}
%where $\Delta_{i}=r^{*}-r_{i}$ denotes the gap between the means of the optimal arm and of the $i$-th arm. 

	In the pure exploration thresholding bandit setup the goal is different than minimizing the cumulative regret, that is the total loss suffered by the learner for not selecting the optimal arm throughout the time horizon $T$. Here the learning algorithm is provided with a threshold $\tau$ and it has to output all such arms $i$ whose $r_{i}$ is above $\tau$ after $T$ rounds. This is a specific instance of combinatorial pure exploration where the learning algorithm can explore as much as possible given a fixed horizon $T$ and not be concerned with the usual exploration-exploitation dilemma. Formally we can define a set $S_{\tau}=\lbrace i\in A: r_{i}\geq \tau \rbrace$ and the complementary set $S_{\tau}^{C}=\lbrace i\in A: r_{i} < \tau \rbrace$. Also we define $\hat{S}_{\tau}=\hat{S}_{\tau}(T)\subset A$ and its complementary set $\hat{S}_{\tau}^{C}$ as the recommendation of the learning algorithm after $T$ rounds. Given such sets exists, the performance of the learning agent is measured by how much accuracy it can discriminate between $S_{\tau}$ and $S_{\tau}^{C}$ after time horizon $T$. The loss $\Ls$ is defined as:-
\begin{align*}
\Ls (T) = I\big(\lbrace S_{\tau}\cap \hat{S}_{\tau}^{C}\neq \emptyset\rbrace    \cup    \lbrace\hat{S}_{\tau}\cap S_{\tau}^{C}\neq \emptyset\rbrace\big)
\end{align*}			
The goal of the learning agent is to minimize $\Ls (T)$. So, the expected loss after $T$ rounds is,
\begin{align*}
\E[\Ls(T)] = \Pb\big(\lbrace S_{\tau}\cap \hat{S}_{\tau}^{C} \neq \emptyset \rbrace  \cup   \lbrace \hat{S}_{\tau}\cap S_{\tau}^{C} \neq \emptyset\rbrace\big)
\end{align*}
which we can say is the probability of making mistake, that is whether the learning agent at the end of round $T$ rejects arms from $S_{\tau}$ or accepts arms from $S_{\tau}^{C}$ in its final recommendation. 

%Also, we are looking at an anytime algorithm, so the knowledge of $T$ may not be known to the learner.
