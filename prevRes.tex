Significant amount of literature is available on the stochastic MAB setting with respect to minimizing the cumulative regret. While the seminal work of \cite{robbins1952some}, \cite{thompson1933likelihood},  and \cite{lai1985asymptotically} prove asymptotic lower bounds on the cumulative regret, the more recent work of \cite{auer2002finite} propose the UCB1 algorithm that provides finite time-horizon guarantees. % with a single optimal arm. 
%
% which gives us an asymptotic lower bound on the cumulative regret we come to the UCB1 algorithm in  \cite{auer2002finite}. 
%
Subsequent work such as \cite{audibert2009minimax} and \cite{auer2010ucb} have improved the upper bounds on the cumulative regret. The authors in \cite{auer2010ucb} have proposed a \emph{round-based}\footnote{An algorithm is said to be \textit{round-based} if it pulls all the arms equal number of times in each round, and then proceeds to eliminate one or more arms that it identifies to be sub-optimal.} version of the UCB algorithm, referred to as UCB-Improved. Of special mention is the work of \cite{audibert2009exploration} where the authors have introduced a \emph{variance-aware} UCB algorithm, referred to as UCB-V; it is shown that the algorithms that take into account variance estimation along with mean estimation tends to perform better than the algorithms that solely focuses on mean estimation, for instance, such as UCB1.
For a more detail survey of literature on UCB algorithms, we refer the reader to \cite{bubeck2012regret}. 


%An early work involving a bandit setup is \cite{thompson1933likelihood}, where the author deals with the problem of choosing between two treatments to administer on patients who come in sequentially. Following the seminal work of  \cite{robbins1952some}, bandit algorithms have been extensively studied in a variety of applications. From a theoretical standpoint, an asymptotic lower bound for the regret was established in \cite{lai1985asymptotically}. Several other works such as \cite{auer2002finite},  \cite{audibert2009minimax} and \cite{auer2010ucb} have shown results for minimizing cumulative regret in stochastic bandit setup whereas works such as \cite{auer2002nonstochastic} have concentrated on adversarial bandit setup.
	
%	In the pure exploration setup, a significant amount of research has been done on finding the best arm(s) from a set of arms. 

In this work we are particularly interested in \emph{pure-exploration MABs},  where the focus in primarily on simple regret rather than the cumulative regret. The relationship between cumulative regret and simple regret is proved in \cite{bubeck2011pure} where the authors prove that minimizing the simple regret necessarily results in maximizing the cumulative regret.
The pure exploration problem has been explored  mainly under the following two settings:
	
	\emph{1. Fixed Budget setting:} Here the learning algorithm has to suggest the best arm(s) within a fixed time-horizon $T$, that is usually given as an input. The objective is to maximize the probability of returning the best arm(s).  This is the scenario we consider in our paper. In \cite{audibert2010best} the authors propose the  UCBE and the Successive Reject (SR) algorithm, and prove simple-regret guarantees for the problem of identifying the single best arm.  In the combinatorial fixed budget setup \cite{gabillon2011multi} propose the Gap-E and Gap-EV algorithms that suggests, with high probability, the best $m$ 
	% (given as input)
	 arms at the end of the time budget. Similarly, \cite{bubeck2013multiple} introduce the  Successive Accept Reject (SAR) algorithm, which is an extension of the SR algorithm; SAR is a round based algorithm whereby at the end of each round an arm is either accepted or rejected (based on certain confidence conditions) until the top $m$ arms are suggested at the end of the budget with high probability. A similar combinatorial setup was explored in \cite{chen2014combinatorial} where the authors propose the Combinatorial Successive Accept Reject (CSAR) algorithm, which is similar in concept to SAR but with a more general setup. 

	\emph{2. Fixed Confidence setting:} In this setting the learning algorithm has to suggest the best arm(s) with a fixed confidence (given as input) with as fewer number of attempts as possible. The single best arm identification has been studied in \cite{even2006action}, while for the combinatorial setup \cite{kalyanakrishnan2012pac} have proposed the LUCB algorithm which, on termination, returns  $m$ arms which are at least $\epsilon$ close to the true top-$m$ arms with probability at least $1-\delta$. For a detail survey of this setup we refer the reader to \cite{jamieson2014best}. 

Apart from these two settings some unified approach has also been suggested in \cite{gabillon2012best} which proposes the algorithms UGapEb and UGapEc which can work in both the above two settings. The thresholding bandit problem is a specific instance of the pure exploration setup of \cite{chen2014combinatorial}. In the latest work of \cite{locatelli2016optimal} Anytime Parameter-Free Thresholding (APT) algorithm comes up with an improved anytime guarantee than CSAR for the thresholding bandit problem.	
	
	
%	\emph{1. Fixed Budget setting:} In this setting the learning algorithm has to suggest the best arm(s) within a fixed number of attempts that is given as an input. The objective here is to maximize the probability of returning the best arm(s). One of the foremost papers to deal with single best arm identification is \cite{audibert2009exploration} where the authors come up with the algorithm UCBE and Successive Reject(SR) with simple regret guarantees. The relationship between cumulative regret and simple regret is proved in \cite{bubeck2011pure} where the authors prove that minimizing the simple regret necessarily results in maximizing the cumulative regret. In the combinatorial fixed budget setup \cite{gabillon2011multi} come up with Gap-E and Gap-EV algorithm which suggests the best $m$ (given as input) arms at the end of the budget with high probability. Similarly, \cite{bubeck2013multiple} comes up with the algorithm Successive Accept Reject(SAR) which is an extension of the SR algorithm. SAR is a round based algorithm whereby at the end of round an arm is either accepted or rejected based on certain conditions till the required top $m$ arms are suggested at the end of the budget with high probability. 
%
%	\emph{2 Fixed Confidence setting:} In this setting the the learning algorithm has to suggest the best arm(s) with a fixed (given as input) confidence with as less number of attempts as possible. The single best arm identification has been handled in \cite{even2006action} where they come up with an algorithm called Successive Elimination (SE) which comes up with an arm that is $\epsilon$ close to the optimal arm. In the combinatorial setup recently \cite{kalyanakrishnan2012pac} have suggested the LUCB algorithm which on termination returns $m$ arms which are atleast $\epsilon$ close to the true top $m$ arms with $1-\delta$ probability.
%
%	Apart from these two settings some unified approach has also been suggested in \cite{gabillon2012best} which proposes the algorithms UGapEb and UGapEc which can work in both the above two settings. A similar combinatorial setup was also explored in \cite{chen2014combinatorial} where the authors come up with more similarities and dissimilarities between these two settings in a more general setup. In their work, the learning algorithm, called Combinatorial Successive Accept Reject (CSAR) is similar to SAR with a more general setup. The thresholding bandit problem is a specific instance of the pure exploration setup of \cite{chen2014combinatorial}. In the latest work in \cite{locatelli2016optimal} the algorithm Anytime Parameter-Free Thresholding (APT) algorithm comes up with a better anytime guarantee than CSAR for the thresholding bandit problem.