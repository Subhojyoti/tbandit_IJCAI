%Bandit problems and their variants have been studied in various domains under different names. An original motivation to the problem is seen in \cite{thompson1933likelihood}, where the author looks at a clinical trials problem of administering two treatments to patients who come in sequentially. Subsequently, various studies such as \cite{robbins1952some} and \cite{lai1985asymptotically}, popularize this broad problem statement under the topic of  \textit{sequential design of experiments}. The later go on  prove an asymptotic lower bound for the regret. The use of an Upper Confidence Bounds(UCB) as a strategy to select arms is first mentioned in the seminal paper by \cite{auer2002finite}, which is also accompanied with a proof on the regret bound. UCB policies are simple to implement and computationally efficient as they do not require large amounts of memory or complex policies of deciding which action to take at which state. A further modification of the UCB algorithm is proposed in \cite{auer2010ucb}, where a round based variant of the UCB algorithm  called UCB Revisited/UCB-Improved. In this manuscript we will call this as UCB-Revisited. All of these papers considered only the stationary distribution version of the bandit problem, that is the distribution from which the rewards are sampled remain fixed over time, but each arm could have a different mean and variance. This is the same context studied in our paper. It is also noteworthy that some UCB algorithms make no explicit variance estimation, such as UCB-1, UCB-2 by \cite{auer2002finite}, and UCB-Revisited by \cite{auer2010ucb}. However, other contributions such as UCB-Normal by \cite{auer2002finite} and UCB-Variance by \cite{audibert2009exploration} do make explicit variance estimates.  Similar to the algorithms UCB-1, UCB-2 and UCB-Revisited, this study also does not use any variance estimation. In fact, the experiments we consider in our study assume that the rewards for each arm come from unique stationary distribution which have different means but the same variance. Various other modifications to the problem statement have also been studied. One example is \citep{bubeck2013bounded}, which studies the stochastic multi-armed bandit problem where the value $r^{*}$ of the optimal arm is known, as a well as a positive lower bound on the smallest positive gap that is the $\min_{i\in A}{\Delta}_{i}, \forall i\in A$.
% 
%
%\paragraph{}The use of round based or phased algorithms, which is at the kernel of the proposed algorithm, can be seen in other studies belonging to the stochastic bandit literature. One variant was originally proposed by \cite{even2006action}, where they come up with algorithms called Successive Elimination and Median Elimination and provide PAC(Probably Approximately Correct) guarantees for them. In PAC guarantee algorithms the learning agent comes up with an $\epsilon$-optimal arm with $\delta$ error probability. Even UCB algorithms can be divided into non-round based and round-based; UCB1 falls under the former category, whereas UCB-Revisited falls under the latter. In the non-round based class of UCB algorithms the agent behaves greedily over a reward function combined with an confidence interval term which creates a fine balance between exploration and exploitation. However, in the round based algorithms, this balance is achieved in another way. Typically, in these algorithms the learning agent deletes some sub-optimal arm(s) at each round and is left with just one arm at the end, which is outputed with a certain error probability. Other approaches to phase wise elimination have also been explored under modified problem statements, an example of this is the Successive Reject algorithm which looks at the budgeted bandit scenario \cite{audibert2010best}. 
%
%\paragraph{}A phase-wise separation of exploration-exploitation is also tackled in \cite{perchet2015batched} which points out that FDA (Food and Drug Administration-USA) also use these type of methods to conduct clinical trials that is having a pilot trial, full trial and then diffusion into larger population. The paper also comes up with a Explore-Then-Commit policy in a batched bandit scenario. 
%%\cite{bubeck2013bounded}, studied  the stochastic multi-armed bandit problem when one knows the value $r^{*}$ of an optimal arm, as a well as a positive lower bound on the smallest positive gap that is the $\min_{i\in A}{\Delta}_{i}, \forall i\in A$.

	A significant amount of work has been done on the stochastic multi-armed bandit setting regarding minimizing cumulative regret with a single optimal arm. For a survey of such works we refer the reader to \cite{bubeck2012regret}. An early work involving a bandit setup is \cite{thompson1933likelihood}, where the author deals with the problem of choosing between two treatments to administer on patients who come in sequentially. Following the seminal work of  \cite{robbins1952some}, bandit algorithms have been extensively studied in a variety of applications. From a theoretical standpoint, an asymptotic lower bound for the regret was established in \cite{lai1985asymptotically}. Several other works such as \cite{auer2002finite},  \cite{audibert2009minimax} and \cite{auer2010ucb} have shown results for minimizing cumulative regret in stochastic bandit setup whereas works such as \cite{auer2002nonstochastic} have concentrated on adversarial bandit setup.
	
	
%	 An early work involving a bandit setup is \cite{thompson1933likelihood}, where the author deals the problem of choosing between two treatments to administer on patients who come in sequentially. Following the seminal work of  \cite{robbins1952some}, bandit algorithms have been extensively studied in a variety of applications. 
%From a theoretical standpoint, an asymptotic lower bound for the regret was established in \cite{lai1985asymptotically}. In particular, it was shown that for any consistent allocation strategy, we have
%$\liminf_{T \to \infty}\frac{\E[R_{T}]}{\log T}\geq\sum_{\{i:r_{i}<r^{*}\}}\dfrac{(r^{*}-r_{i})}{D(p_{i}||p^{*})},$
%where $D(p_{i}||p^{*})$ is the Kullback-Leibler divergence between the reward densities $p_{i}$ and $p^{*}$, corresponding to arms with mean $r_{i}$ and $r^{*}$, respectively.

%	There have been several algorithms with strong regret guarantees. The foremost among them is UCB1 by  \cite{auer2002finite}, which has a regret upper bound of $O\bigg(\dfrac{K\log T}{\Delta}\bigg)$, where $\Delta = \min_{i:\Delta_i>0} \Delta_i$. This result is asymptotically order-optimal for the class of distributions considered. However, the worst case gap independent regret bound of UCB1  can be as bad as $O \bigg(\sqrt{TK\log T}\bigg)$.  In \cite{audibert2009minimax}, the authors propose the MOSS algorithm and establish that the worst case regret of MOSS is $O\bigg(\sqrt{TK}\bigg)$ which improves upon UCB1 by a factor of order $\sqrt{\log T}$. However, the gap-dependent regret of MOSS is  $O\left(\dfrac{K^{2}\log\left(T\Delta^{2}/K\right)}{\Delta}\right)$ and in certain regimes, this can be worse than even UCB1 (see \cite{audibert2009minimax},\cite{lattimore2015optimally}). The UCB-Improved algorithm, proposed in \cite{auer2010ucb}, is a round-based algorithm\footnote{An algorithm is \textit{round-based} if it pulls all the arms equal number of times in each round and then proceeds to eliminate one or more arms that it identifies to be sub-optimal.} variant of UCB1 that has a gap-dependent regret bound of $O\bigg(\dfrac{K\log T\Delta^{2}}{\Delta}\bigg)$, which is better than that of UCB1. On the other hand, the worst case regret of UCB-Improved is $O\bigg(\sqrt{TK\log K}\bigg)$. 

	In the pure exploration setup, a significant amount of research has been done on finding the best arm(s) from a set of arms. The pure exploration setup has been explored in mainly two settings:-
\begin{enumerate}
\item Fixed Budget setting: In this setting the learning algorithm has to suggest the best arm(s) within a fixed number of attempts that is given as an input. The objective here is to maximize the probability of returning the best arm(s). One of the foremost papers to deal with single best arm identification is \cite{audibert2009exploration} where the authors come up with the algorithm UCBE and Successive Reject(SR) with simple regret guarantees. The relationship between cumulative regret and simple regret is proved in \cite{bubeck2011pure} where the authors prove that minimizing the simple regret necessarily results in maximizing the cumulative regret. In the combinatorial fixed budget setup \cite{gabillon2011multi} come up with Gap-E and Gap-EV algorithm which suggests the best $m$ (given as input) arms at the end of the budget with high probability. Similarly, \cite{bubeck2013multiple} comes up with the algorithm Successive Accept Reject(SAR) which is an extension of the SR algorithm. SAR is a round based algorithm whereby at the end of round an arm is either accepted or rejected based on certain conditions till the required top $m$ arms are suggested at the end of the budget with high probability. 
\item Fixed Confidence setting: In this setting the the learning algorithm has to suggest the best arm(s) with a fixed (given as input) confidence with as less number of attempts as possible. The single best arm identification has been handled in \cite{even2006action} where they come up with an algorithm called Successive Elimination (SE) which comes up with an arm that is $\epsilon$ close to the optimal arm. In the combinatorial setup recently \cite{kalyanakrishnan2012pac} have suggested the LUCB algorithm which on termination returns $m$ arms which are atleast $\epsilon$ close to the true top $m$ arms with $1-\delta$ probability.
\end{enumerate}	

	Apart from these two settings some unified approach has also been suggested in \cite{gabillon2012best} which proposes the algorithms UGapEb and UGapEc which can work in both the above two settings. A similar combinatorial setup was also explored in \cite{chen2014combinatorial} where the authors come up with more similarities and dissimilarities between these two settings in a more general setup. In their work, the learning algorithm, called Combinatorial Successive Accept Reject (CSAR) is similar to SAR with a more general setup. The thresholding bandit problem is a specific instance of the pure exploration setup of \cite{chen2014combinatorial}. In the latest work in \cite{locatelli2016optimal} the algorithm Anytime Parameter-Free Thresholding (APT) algorithm comes up with a better anytime guarantee than CSAR for the thresholding bandit problem.