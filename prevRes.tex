A significant amount of work has been done on the stochastic MAB setting regarding minimizing cumulative regret with a single optimal arm. For a survey of such works we refer the reader to \cite{bubeck2012regret}. Starting from the early work of \cite{thompson1933likelihood}, \cite{robbins1952some} to \cite{lai1985asymptotically} which gives us an asymptotic lower bound on the cumulative regret we come to the UCB1 algorithm in  \cite{auer2002finite}. Subsequent works such as \cite{audibert2009minimax} and \cite{auer2010ucb} have shown better upper bounds on the cumulative regret. In \cite{auer2010ucb} they propose the UCB-Improved algorithm which is round-based algorithm\footnote{An algorithm is \textit{round-based} if it pulls all the arms equal number of times in each round and then proceeds to eliminate one or more arms that it identifies to be sub-optimal.}. Of special mention is the \cite{audibert2009exploration} where they introduce variance-aware algorithm UCB-V and show that algorithms that take into account variance estimation along with mean estimation tends to perform better than algorithms than solely focuses on mean estimation such as UCB1.


%An early work involving a bandit setup is \cite{thompson1933likelihood}, where the author deals with the problem of choosing between two treatments to administer on patients who come in sequentially. Following the seminal work of  \cite{robbins1952some}, bandit algorithms have been extensively studied in a variety of applications. From a theoretical standpoint, an asymptotic lower bound for the regret was established in \cite{lai1985asymptotically}. Several other works such as \cite{auer2002finite},  \cite{audibert2009minimax} and \cite{auer2010ucb} have shown results for minimizing cumulative regret in stochastic bandit setup whereas works such as \cite{auer2002nonstochastic} have concentrated on adversarial bandit setup.
	
%	In the pure exploration setup, a significant amount of research has been done on finding the best arm(s) from a set of arms. 
	The pure exploration setup has been explored in mainly two settings:-
	
	\emph{1. Fixed Budget setting:} In this setting the learning algorithm has to suggest the best arm(s) within a fixed number of attempts that is given as an input. The objective here is to maximize the probability of returning the best arm(s).  We study this setting in our paper. In \cite{audibert2010best} the authors come up with the algorithm UCBE and Successive Reject(SR) with simple regret guarantees to find the single best arm. The relationship between cumulative regret and simple regret is proved in \cite{bubeck2011pure} where the authors prove that minimizing the simple regret necessarily results in maximizing the cumulative regret. In the combinatorial fixed budget setup \cite{gabillon2011multi} come up with Gap-E and Gap-EV algorithm which suggests the best $m$ (given as input) arms at the end of the budget with high probability. Similarly, \cite{bubeck2013multiple} comes up with the algorithm Successive Accept Reject(SAR) which is an extension of the SR algorithm. SAR is a round based algorithm whereby at the end of a round an arm is either accepted or rejected based on certain conditions till the required top $m$ arms are suggested at the end of the budget with high probability. A similar combinatorial setup was also explored in \cite{chen2014combinatorial} where the authors come up with an algorithm, called Combinatorial Successive Accept Reject (CSAR) which is similar to SAR but with a more general setup. 

	\emph{2. Fixed Confidence setting:} In this setting the the learning algorithm has to suggest the best arm(s) with a fixed confidence (given as input) with as less number of attempts as possible. The single best arm identification has been handled in \cite{even2006action} while in the combinatorial setup \cite{kalyanakrishnan2012pac} have suggested the LUCB algorithm which on termination returns $m$ arms which are atleast $\epsilon$ close to the true top $m$ arms with $1-\delta$ probability. For a survey of this setup we refer the reader to \cite{jamieson2014best}. 

	Apart from these two settings some unified approach has also been suggested in \cite{gabillon2012best} which proposes the algorithms UGapEb and UGapEc which can work in both the above two settings. The thresholding bandit problem is a specific instance of the pure exploration setup of \cite{chen2014combinatorial}. In the latest work in \cite{locatelli2016optimal} the algorithm Anytime Parameter-Free Thresholding (APT) algorithm comes up with a better anytime guarantee than CSAR for the thresholding bandit problem.	
	
	
%	\emph{1. Fixed Budget setting:} In this setting the learning algorithm has to suggest the best arm(s) within a fixed number of attempts that is given as an input. The objective here is to maximize the probability of returning the best arm(s). One of the foremost papers to deal with single best arm identification is \cite{audibert2009exploration} where the authors come up with the algorithm UCBE and Successive Reject(SR) with simple regret guarantees. The relationship between cumulative regret and simple regret is proved in \cite{bubeck2011pure} where the authors prove that minimizing the simple regret necessarily results in maximizing the cumulative regret. In the combinatorial fixed budget setup \cite{gabillon2011multi} come up with Gap-E and Gap-EV algorithm which suggests the best $m$ (given as input) arms at the end of the budget with high probability. Similarly, \cite{bubeck2013multiple} comes up with the algorithm Successive Accept Reject(SAR) which is an extension of the SR algorithm. SAR is a round based algorithm whereby at the end of round an arm is either accepted or rejected based on certain conditions till the required top $m$ arms are suggested at the end of the budget with high probability. 
%
%	\emph{2 Fixed Confidence setting:} In this setting the the learning algorithm has to suggest the best arm(s) with a fixed (given as input) confidence with as less number of attempts as possible. The single best arm identification has been handled in \cite{even2006action} where they come up with an algorithm called Successive Elimination (SE) which comes up with an arm that is $\epsilon$ close to the optimal arm. In the combinatorial setup recently \cite{kalyanakrishnan2012pac} have suggested the LUCB algorithm which on termination returns $m$ arms which are atleast $\epsilon$ close to the true top $m$ arms with $1-\delta$ probability.
%
%	Apart from these two settings some unified approach has also been suggested in \cite{gabillon2012best} which proposes the algorithms UGapEb and UGapEc which can work in both the above two settings. A similar combinatorial setup was also explored in \cite{chen2014combinatorial} where the authors come up with more similarities and dissimilarities between these two settings in a more general setup. In their work, the learning algorithm, called Combinatorial Successive Accept Reject (CSAR) is similar to SAR with a more general setup. The thresholding bandit problem is a specific instance of the pure exploration setup of \cite{chen2014combinatorial}. In the latest work in \cite{locatelli2016optimal} the algorithm Anytime Parameter-Free Thresholding (APT) algorithm comes up with a better anytime guarantee than CSAR for the thresholding bandit problem.